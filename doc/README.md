Extending R/cape: Multivariant Analysis
========================================================

The purpose of this project is to test a new model to extend `R/cape` for the analysis of more than 2 variants simulataneously.

There are two parts to the code:
* `create_data.R`, where we fabricate a dataset given an underlying variant-to-variant interaction network, and
* `get_interactions.R` where we use our new model on the fabricated marker and phenotype data to re-infer the underlying variant interactions.

Using this methodology, we can implement the new model of analysis while testing its robustness as it responds to varying levels of noise that we can systematically introduce into the data.

------

Running the code: an example
========================================================
## create_data.R
Here is an example of the data generated by the script `create_data.R`. We set several parameters in the fabrication of this dataset, most notably
* 1000 individuals,
* 3 phenotypes,
* 3 variants,
* 50% allelic frequency, and
* __No noise__ -- this will be updated shortly

Then, using the underlying interaction matrix $\boldsymbol{A}$, set to
<!-- 

```r
library("MASS")
library("Matrix")
load("~/JAX/simulation/v1.3/bin/full_simulation.RData")
```
 -->

```r
print(dataset$A)
```

```
##      [,1] [,2] [,3]
## [1,]  0.0  0.4  0.0
## [2,]  0.1  0.0  0.3
## [3,]  0.0  0.0  0.0
```

and the main effect matrix

```r
print(params$ME_betas)
```

```
##      [,1] [,2] [,3]
## [1,]  3.0  3.5  4.0
## [2,]  3.5  4.0  3.0
## [3,]  4.0  3.0  3.5
```

we created the $\beta$ matrix.

```r
print(dataset$betas)
```

```
##      [,1] [,2] [,3]
## [1,] 1.00 1.00 1.00
## [2,] 3.00 3.50 4.00
## [3,] 3.50 4.00 3.00
## [4,] 4.00 3.00 3.50
## [5,] 1.70 1.95 1.60
## [6,] 0.00 0.00 0.00
## [7,] 1.20 0.90 1.05
## [8,] 0.48 0.36 0.42
```

Now, using the linear regression model
$$
Y \sim X \cdot \beta + \epsilon,
$$
where $Y$ is the phenotype matrix (columns represent phenotypes, rows invididuals), $X$ is the genotype matrix, and $\beta$ is the weights matrix, we get the following fabricated data:

<!-- colnames(X) <- c("x0", "x1", "x2", "x3", "x4", "x12", "x13", "x14", "x23", "x24", "x34", 
                 "x123", "x124", "x134", "x234", "x1234") -->

```r
colnames(X) <- c("x0", "x1", "x2", "x3", "x12", "x13", "x23", "x123")
head(X)
```

```
##      x0 x1 x2 x3 x12 x13 x23 x123
## [1,]  1  0  1  1   0   0   1    0
## [2,]  1  0  1  1   0   0   1    0
## [3,]  1  1  1  2   1   2   2    2
## [4,]  1  1  2  1   2   1   2    2
## [5,]  1  1  0  1   0   1   0    0
## [6,]  1  1  0  0   0   0   0    0
```

```r
head(Y)
```

```
##       [,1]  [,2]  [,3]
## [1,]  9.70  8.90  8.55
## [2,]  9.70  8.90  8.55
## [3,] 20.56 18.97 19.54
## [4,] 21.76 21.92 20.64
## [5,]  8.00  7.50  8.50
## [6,]  4.00  4.50  5.00
```

------
## get_interactions.R

<!-- 
__TODO: there is currently a problem with delta matrix $\boldsymbol{I}$ generation involving routing and feedback.__ The part in `helper_fn.R` in the `get.I.from.A` method:
```
...
  active_vars <- A[combo,combo]
  routes <- active_vars %^% (k-1) #routes of length k-1
  diag(routes) <- 0
  delta_row <- colSums(routes) 
...
``` -->


To infer the values of the interactions from the marker data $X$ and phenotype data $Y$, we first find the weights $\beta$ of our linear regression model. Since

$$\beta \approx X^{-1} \cdot Y,$$

in `R` we do:

```r
ginv(X) %*% Y
```

```
##           [,1]       [,2]      [,3]
## [1,] 1.000e+00  1.000e+00 1.000e+00
## [2,] 3.000e+00  3.500e+00 4.000e+00
## [3,] 3.500e+00  4.000e+00 3.000e+00
## [4,] 4.000e+00  3.000e+00 3.500e+00
## [5,] 1.700e+00  1.950e+00 1.600e+00
## [6,] 7.772e-16 -1.665e-16 4.621e-15
## [7,] 1.200e+00  9.000e-01 1.050e+00
## [8,] 4.800e-01  3.600e-01 4.200e-01
```

Notice that these values are essentially equal to the original $\beta$ values. That's because we didn't add any noise to the data yet.

Continuing, we want to find the interaction matrix $\boldsymbol{A}$ filled with $\delta$'s that best fits the following system by the least squares criterion.

$$\begin{cases} 
  \beta_{12}^1 = \beta_{1}^1\delta_{21} + \beta_{2}^1\delta_{12} \\\ 
  \beta_{12}^2 = \beta_{1}^2\delta_{21} + \beta_{2}^2\delta_{12} \\\ 
  \ldots \\\ 
  \beta_{123}^2 = \beta_{1}^2(\delta_{21} + \delta_{31} + \delta_{23}\delta_{31} + \delta_{32}\delta_{21}) + \beta_{2}^2(\delta_{12} + \delta_{32} + \delta_{13}\delta_{32} + \delta_{31}\delta_{12}) \\\ 
  \beta_{123}^3 = \beta_{1}^3(\delta_{21} + \delta_{31} + \delta_{23}\delta_{31} + \delta_{32}\delta_{21}) + \beta_{2}^3(\delta_{12} + \delta_{32} + \delta_{13}\delta_{32} + \delta_{31}\delta_{12})
\end{cases}$$
__Note:__ Example system of equations above is for the 3 variant 3 phenotype case.

Enumerating all the cases, we see that we have 12 equations and 6 variables. (In general, we will have $p (2^v - v - 1)$ equations and $2{v \choose 2}$ variables.)

Now, our task is to find the best $\delta$'s to fit the equations. We use a series of four optimization algorithms to look for the minima: 
* BFGS, a gradient search method, 
* Levenberg-Marquardt, a hybrid between gradient descent and Newton's method, 
* Nelder-Mead, a simplex method, and 
* Simulated Annealing, an entropy-based random global search method.

We show the results for each here:
```
solve.bfgs <- bfgs()
```


```r
print(solve.bfgs)
```

```
## $par
## [1]  1.000e-01  2.632e-10  4.000e-01  3.657e-10 -2.064e-10  3.000e-01
## 
## $value
## [1] 8.085e-19
## 
## $counts
## function gradient 
##       86       40 
## 
## $convergence
## [1] 0
## 
## $message
## NULL
```

```r
signif(reshape.with.diag(solve.bfgs$par, n_v), digits=4)
```

```
##           [,1]      [,2]       [,3]
## [1,] 0.000e+00 4.000e-01 -2.064e-10
## [2,] 1.000e-01 0.000e+00  3.000e-01
## [3,] 2.632e-10 3.657e-10  0.000e+00
```

```
solve.lev.marq <- lev.marq()
```


```r
print(solve.lev.marq)
```

```
## Nonlinear regression via the Levenberg-Marquardt algorithm
## parameter estimates: 0.100000000789061, -1.28844058347826e-09, 0.399999999194725, -3.19963929952106e-09, 1.26249932914136e-09, 0.300000003569382 
## residual sum-of-squares: 2.78e-17
## reason terminated: Relative error between `par' and the solution is at most `ptol'.
```

```r
signif(reshape.with.diag(solve.lev.marq$par, n_v), digits=4)
```

```
##            [,1]     [,2]      [,3]
## [1,]  0.000e+00  4.0e-01 1.262e-09
## [2,]  1.000e-01  0.0e+00 3.000e-01
## [3,] -1.288e-09 -3.2e-09 0.000e+00
```

```
solve.nelder.mead <- nelder.mead()
```


```r
print(solve.nelder.mead)
```

```
## $par
## [1]  9.982e-02  8.360e-05  4.001e-01 -6.436e-05 -3.736e-05  3.000e-01
## 
## $value
## [1] 6.142e-07
## 
## $counts
## function gradient 
##     1457       NA 
## 
## $convergence
## [1] 0
## 
## $message
## NULL
```

```r
signif(reshape.with.diag(solve.nelder.mead$par, n_v), digits=4)
```

```
##           [,1]       [,2]       [,3]
## [1,] 0.0000000  4.001e-01 -3.736e-05
## [2,] 0.0998200  0.000e+00  3.000e-01
## [3,] 0.0000836 -6.436e-05  0.000e+00
```

```
solve.sim.anneal <- sim.anneal()
```


```r
print(solve.sim.anneal)
```

```
## $par
## [1]  0.17651  0.17997  0.27681  0.02929 -0.19097  0.27415
## 
## $value
## [1] 0.1771
## 
## $counts
## function gradient 
##    20000       NA 
## 
## $convergence
## [1] 0
## 
## $message
## NULL
```

```r
signif(reshape.with.diag(solve.sim.anneal$par, n_v), digits=4)
```

```
##        [,1]    [,2]    [,3]
## [1,] 0.0000 0.27680 -0.1910
## [2,] 0.1765 0.00000  0.2741
## [3,] 0.1800 0.02929  0.0000
```

We see that the results in the solution vector (called `par`, for the parameter vector) is quite similar for BFGS and Nelder Mead. This is reassuring as BFGS is a gradient method (and its performance generally depends on the initial guess), whereas Nelder-Mead is a global search method whose ability to find a minimum is less dependent on the intialization point. Thus, our method can show us with some confidence where the global minimum is.

Furthermore, we see that our method has done well in finding the global minimum in this case, as the output matrices are very close to the original seeded interaction matrix.
