{"name":"Epistasis-simulation","tagline":"creating simulated data to test statistical techniques to detect gene-gene interactions.","body":"Extending R/cape: Multivariant Analysis\r\n========================================================\r\n\r\nThe purpose of this project is to test a new model to extend `R/cape` for the analysis of more than 2 variants simulataneously.\r\n\r\nThere are two parts to the code:\r\n* `create_data.R`, where we fabricate a dataset given an underlying variant-to-variant interaction network, and\r\n* `get_interactions.R` where we use our new model on the fabricated marker and phenotype data to re-infer the underlying variant interactions.\r\n\r\nUsing this methodology, we can implement the new model of analysis while testing its robustness as it responds to varying levels of noise that we can systematically introduce into the data.\r\n\r\n------\r\n\r\nRunning the code: an example\r\n========================================================\r\n## create_data.R\r\nHere is an example of the data generated by the script `create_data.R`. We set several parameters in the fabrication of this dataset, most notably\r\n* 1000 individuals,\r\n* 3 phenotypes,\r\n* 3 variants,\r\n* 50% allelic frequency, and\r\n* __No noise__ -- this will be updated shortly\r\n\r\nThen, using the underlying interaction matrix $\\boldsymbol{A}$, set to\r\n<!-- \r\n\r\n```r\r\nlibrary(\"MASS\")\r\nlibrary(\"Matrix\")\r\nload(\"~/JAX/simulation/v1.2/bin/full_simulation.RData\")\r\n```\r\n\r\n```\r\n## Warning: cannot open compressed file\r\n## '/Users/mingya/JAX/simulation/v1.2/bin/full_simulation.RData', probable\r\n## reason 'No such file or directory'\r\n```\r\n\r\n```\r\n## Error: cannot open the connection\r\n```\r\n -->\r\n\r\n```r\r\nprint(A)\r\n```\r\n\r\n```\r\n##      [,1] [,2] [,3]\r\n## [1,]  0.0  0.4  0.0\r\n## [2,]  0.1  0.0  0.3\r\n## [3,]  0.0  0.0  0.0\r\n```\r\n\r\nand the main effect matrix\r\n\r\n```r\r\nprint(ME_betas)\r\n```\r\n\r\n```\r\n##      [,1] [,2] [,3]\r\n## [1,]  3.0  3.5  4.0\r\n## [2,]  3.5  4.0  3.0\r\n## [3,]  4.0  3.0  3.5\r\n```\r\n\r\nwe created the $\\beta$ matrix.\r\n\r\n```r\r\nprint(betas)\r\n```\r\n\r\n```\r\n##      [,1] [,2] [,3]\r\n## [1,] 1.00 1.00 1.00\r\n## [2,] 3.00 3.50 4.00\r\n## [3,] 3.50 4.00 3.00\r\n## [4,] 4.00 3.00 3.50\r\n## [5,] 1.70 1.95 1.60\r\n## [6,] 0.00 0.00 0.00\r\n## [7,] 1.20 0.90 1.05\r\n## [8,] 0.48 0.36 0.42\r\n```\r\n\r\nNow, using the linear regression model\r\n$$\r\nY \\sim X \\cdot \\beta + \\epsilon,\r\n$$\r\nwhere $Y$ is the phenotype matrix (columns represent phenotypes, rows invididuals), $X$ is the genotype matrix, and $\\beta$ is the weights matrix, we get the following fabricated data:\r\n\r\n\r\n```r\r\ncolnames(X) <- c(\"x0\", \"x1\", \"x2\", \"x3\", \"x12\", \"x13\", \"x23\", \"x123\")\r\nhead(X)\r\n```\r\n\r\n```\r\n##      x0 x1 x2 x3 x12 x13 x23 x123\r\n## [1,]  1  2  2  2   4   4   4    8\r\n## [2,]  1  2  1  0   2   0   0    0\r\n## [3,]  1  1  1  1   1   1   1    1\r\n## [4,]  1  0  1  2   0   0   2    0\r\n## [5,]  1  2  2  0   4   0   0    0\r\n## [6,]  1  0  1  1   0   0   1    0\r\n```\r\n\r\n```r\r\nhead(Y)\r\n```\r\n\r\n```\r\n##       [,1]  [,2]  [,3]\r\n## [1,] 37.44 36.28 35.96\r\n## [2,] 13.90 15.90 15.20\r\n## [3,] 14.88 14.71 14.57\r\n## [4,] 14.90 12.80 13.10\r\n## [5,] 20.80 23.80 21.40\r\n## [6,]  9.70  8.90  8.55\r\n```\r\n\r\n------\r\n## get_interactions.R\r\n\r\n<!-- \r\n__TODO: there is currently a problem with delta matrix $\\boldsymbol{I}$ generation involving routing and feedback.__ The part in `helper_fn.R` in the `get.I.from.A` method:\r\n```\r\n...\r\n  active_vars <- A[combo,combo]\r\n  routes <- active_vars %^% (k-1) #routes of length k-1\r\n  diag(routes) <- 0\r\n  delta_row <- colSums(routes) \r\n...\r\n``` -->\r\n\r\n\r\nTo infer the values of the interactions from the marker data $X$ and phenotype data $Y$, we first find the weights $\\beta$ of our linear regression model. Since\r\n\r\n$$\\beta \\approx X^{-1} \\cdot Y,$$\r\n\r\nin `R` we do:\r\n\r\n```r\r\nginv(X) %*% Y\r\n```\r\n\r\n```\r\n##           [,1]      [,2]      [,3]\r\n## [1,] 1.000e+00 1.000e+00 1.000e+00\r\n## [2,] 3.000e+00 3.500e+00 4.000e+00\r\n## [3,] 3.500e+00 4.000e+00 3.000e+00\r\n## [4,] 4.000e+00 3.000e+00 3.500e+00\r\n## [5,] 1.700e+00 1.950e+00 1.600e+00\r\n## [6,] 3.905e-14 3.987e-14 4.059e-14\r\n## [7,] 1.200e+00 9.000e-01 1.050e+00\r\n## [8,] 4.800e-01 3.600e-01 4.200e-01\r\n```\r\n\r\nNotice that these values are essentially equal to the original $\\beta$ values. That's because we didn't add any noise to the data yet.\r\n\r\nContinuing, we want to find the interaction matrix $\\boldsymbol{A}$ filled with $\\delta$'s that best fits the following system by the least squares criterion.\r\n\r\n$$\\begin{cases} \r\n  \\beta_{12}^1 = \\beta_{1}^1\\delta_{21} + \\beta_{2}^1\\delta_{12} \\\\\\ \r\n  \\beta_{12}^2 = \\beta_{1}^2\\delta_{21} + \\beta_{2}^2\\delta_{12} \\\\\\ \r\n  \\ldots \\\\\\ \r\n  \\beta_{123}^2 = \\beta_{1}^2(\\delta_{21} + \\delta_{31} + \\delta_{23}\\delta_{31} + \\delta_{32}\\delta_{21}) + \\beta_{2}^2(\\delta_{12} + \\delta_{32} + \\delta_{13}\\delta_{32} + \\delta_{31}\\delta_{12}) \\\\\\ \r\n  \\beta_{123}^3 = \\beta_{1}^3(\\delta_{21} + \\delta_{31} + \\delta_{23}\\delta_{31} + \\delta_{32}\\delta_{21}) + \\beta_{2}^3(\\delta_{12} + \\delta_{32} + \\delta_{13}\\delta_{32} + \\delta_{31}\\delta_{12}) \r\n\\end{cases}$$\r\n\r\nEnumerating all the cases, we see that we have 12 equations and 6 variables. (In general, we will have $p (2^v - v - 1)$ equations and ${v \\choose 2}$ variables.)\r\n\r\nNow, our task is to find the best $\\delta$'s to fit the 12 equations. We use a series of three optimization algorithms to look for the minima: BFGS, a gradient search method, Nelder-Mead, a simplex method, and Simulated Annealing, an entropy-based random global search method.\r\n\r\nWe show the results for each here:\r\n```\r\nsolve.bfgs <- bfgs()\r\n```\r\n\r\n\r\n```r\r\nprint(solve.bfgs)\r\n```\r\n\r\n```\r\n## $par\r\n## [1]  1.000e-01  2.632e-10  4.000e-01  3.657e-10 -2.064e-10  3.000e-01\r\n## \r\n## $value\r\n## [1] 8.085e-19\r\n## \r\n## $counts\r\n## function gradient \r\n##       86       40 \r\n## \r\n## $convergence\r\n## [1] 0\r\n## \r\n## $message\r\n## NULL\r\n```\r\n\r\n```r\r\nsignif(reshape.with.diag(solve.bfgs$par, n_v), digits=4)\r\n```\r\n\r\n```\r\n##           [,1]      [,2]       [,3]\r\n## [1,] 0.000e+00 4.000e-01 -2.064e-10\r\n## [2,] 1.000e-01 0.000e+00  3.000e-01\r\n## [3,] 2.632e-10 3.657e-10  0.000e+00\r\n```\r\n\r\n```\r\nsolve.nelder.mead <- nelder.mead()\r\n```\r\n\r\n\r\n```r\r\nprint(solve.nelder.mead)\r\n```\r\n\r\n```\r\n## $par\r\n## [1]  9.982e-02  8.360e-05  4.001e-01 -6.436e-05 -3.736e-05  3.000e-01\r\n## \r\n## $value\r\n## [1] 6.142e-07\r\n## \r\n## $counts\r\n## function gradient \r\n##     1457       NA \r\n## \r\n## $convergence\r\n## [1] 0\r\n## \r\n## $message\r\n## NULL\r\n```\r\n\r\n```r\r\nsignif(reshape.with.diag(solve.nelder.mead$par, n_v), digits=4)\r\n```\r\n\r\n```\r\n##           [,1]       [,2]       [,3]\r\n## [1,] 0.0000000  4.001e-01 -3.736e-05\r\n## [2,] 0.0998200  0.000e+00  3.000e-01\r\n## [3,] 0.0000836 -6.436e-05  0.000e+00\r\n```\r\n\r\n```\r\nsolve.sim.anneal <- sim.anneal()\r\n```\r\n\r\n\r\n```r\r\nprint(solve.sim.anneal)\r\n```\r\n\r\n```\r\n## $par\r\n## [1]  0.1881  0.2623  0.3234  0.1271 -0.2809  0.2055\r\n## \r\n## $value\r\n## [1] 0.2264\r\n## \r\n## $counts\r\n## function gradient \r\n##    20000       NA \r\n## \r\n## $convergence\r\n## [1] 0\r\n## \r\n## $message\r\n## NULL\r\n```\r\n\r\n```r\r\nsignif(reshape.with.diag(solve.sim.anneal$par, n_v), digits=4)\r\n```\r\n\r\n```\r\n##        [,1]   [,2]    [,3]\r\n## [1,] 0.0000 0.3234 -0.2809\r\n## [2,] 0.1881 0.0000  0.2055\r\n## [3,] 0.2623 0.1271  0.0000\r\n```\r\n\r\nWe see that the results in the solution vector (called `par`, for the parameter vector) is quite similar for BFGS and Nelder Mead. This is reassuring as BFGS is a gradient method (and its performance generally depends on the initial guess), whereas Nelder-Mead is a global search method whose ability to find a minimum is less dependent on the intialization point. Thus, our method can show us with some confidence where the global minimum is.\r\n\r\nFurthermore, we see that our method has done well in finding the global minimum in this case, as the output matrices are very close to the original seeded interaction matrix.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}